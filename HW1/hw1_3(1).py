# -*- coding: utf-8 -*-
"""HW1-3(1).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1eekqWiwVChzOlAGnVcKGBPV1CaC2UidV
"""

import torch
import torch.nn as nn
import torch.optim as optim
from torchvision import datasets, transforms
import matplotlib.pyplot as plt

# Function to create the neural network model with variable hidden units
def create_model(hidden_units):
    class Net(nn.Module):
        def __init__(self):
            super(Net, self).__init__()
            self.fc1 = nn.Linear(784, hidden_units)  # input layer (28x28 images) -> hidden layer
            self.fc2 = nn.Linear(hidden_units, hidden_units)  # hidden layer -> hidden layer
            self.fc3 = nn.Linear(hidden_units, 10)  # hidden layer -> output layer

        def forward(self, x):
            x = torch.relu(self.fc1(x))
            x = torch.relu(self.fc2(x))
            x = self.fc3(x)
            return x

    return Net()

# Count the number of parameters in the model
def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)

# Dataset loading for MNIST
transform = transforms.Compose([transforms.ToTensor()])
trainset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)
testset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)

# Random label dataset
class RandomLabelDataset(torch.utils.data.Dataset):
    def __init__(self, dataset):
        self.dataset = dataset
        self.labels = torch.randperm(len(dataset)) % 10  # Shuffle labels randomly

    def __getitem__(self, index):
        data, _ = self.dataset[index]
        return data, self.labels[index]

    def __len__(self):
        return len(self.dataset)

# Randomly shuffle labels
random_trainset = RandomLabelDataset(trainset)
random_testset = RandomLabelDataset(testset)

trainloader = torch.utils.data.DataLoader(random_trainset, batch_size=64, shuffle=True)
testloader = torch.utils.data.DataLoader(random_testset, batch_size=64, shuffle=False)

# Hidden unit sizes for different models
hidden_units_list = [16, 32, 64, 128, 256, 512, 1024, 2048, 4096, 8192]

# Initialize lists to store the number of parameters, training loss, testing loss, accuracy
num_parameters = []
train_losses = []
test_losses = []
train_accs = []
test_accs = []

# Training settings
num_epochs = 10
learning_rate = 0.001
criterion = nn.CrossEntropyLoss()

# Loop through different model configurations
for hidden_units in hidden_units_list:
    # Create model and optimizer
    model = create_model(hidden_units)
    optimizer = optim.SGD(model.parameters(), lr=learning_rate)

    # Record the number of parameters
    params_count = count_parameters(model)
    num_parameters.append(params_count)

    # Train the model
    for epoch in range(num_epochs):
        model.train()
        for x, y in trainloader:
            x = x.view(-1, 784)  # Flatten the image
            optimizer.zero_grad()
            output = model(x)
            loss = criterion(output, y)
            loss.backward()
            optimizer.step()

    # Evaluate on training set
    model.eval()
    train_loss = 0
    correct = 0
    with torch.no_grad():
        for x, y in trainloader:
            x = x.view(-1, 784)
            output = model(x)
            loss = criterion(output, y)
            train_loss += loss.item()
            _, predicted = torch.max(output, 1)
            correct += (predicted == y).sum().item()
    train_loss /= len(trainloader)
    train_acc = correct / len(trainloader.dataset)

    # Evaluate on test set
    test_loss = 0
    correct = 0
    with torch.no_grad():
        for x, y in testloader:
            x = x.view(-1, 784)
            output = model(x)
            loss = criterion(output, y)
            test_loss += loss.item()
            _, predicted = torch.max(output, 1)
            correct += (predicted == y).sum().item()
    test_loss /= len(testloader)
    test_acc = correct / len(testloader.dataset)

    # Store losses and accuracies
    train_losses.append(train_loss)
    test_losses.append(test_loss)
    train_accs.append(train_acc)
    test_accs.append(test_acc)

    print(f'Hidden Units: {hidden_units}, Params: {params_count}, Train Loss: {train_loss:.4f}, Test Loss: {test_loss:.4f}, Train Acc: {train_acc:.4f}, Test Acc: {test_acc:.4f}')

# Plot loss and accuracy as a function of number of parameters
plt.figure(figsize=(12, 6))

plt.subplot(1, 2, 1)
plt.plot(num_parameters, train_losses, label='Training Loss')
plt.plot(num_parameters, test_losses, label='Testing Loss')
plt.xscale('log')
plt.xlabel('Number of Parameters')
plt.ylabel('Loss')
plt.title('Loss vs. Number of Parameters')
plt.legend()

plt.subplot(1, 2, 2)
plt.plot(num_parameters, train_accs, label='Training Accuracy')
plt.plot(num_parameters, test_accs, label='Testing Accuracy')
plt.xscale('log')
plt.xlabel('Number of Parameters')
plt.ylabel('Accuracy')
plt.title('Accuracy vs. Number of Parameters')
plt.legend()

plt.show()

