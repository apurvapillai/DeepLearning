# -*- coding: utf-8 -*-
"""flatness vs generalization.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1WIjDJFAvT_uDK2aM4L1pey7M_1VypLJB
"""

import torch
import torch.nn as nn
import torch.optim as optim
from torchvision import datasets, transforms
import numpy as np
import matplotlib.pyplot as plt

# Define the device (GPU or CPU)
device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")

# Define the data transforms
transform = transforms.Compose([transforms.ToTensor(),
                                transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])

# Load the CIFAR10 dataset
trainset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)
testset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)

# Define the neural network model with dropout
class Net(nn.Module):
    def __init__(self):
        super(Net, self).__init__()
        self.conv1 = nn.Conv2d(3, 6, 5)
        self.pool = nn.MaxPool2d(2, 2)
        self.conv2 = nn.Conv2d(6, 16, 5)
        self.fc1 = nn.Linear(16 * 5 * 5, 120)
        self.fc2 = nn.Linear(120, 84)
        self.fc3 = nn.Linear(84, 10)
        self.dropout = nn.Dropout(0.5)  # Add dropout to prevent overfitting

    def forward(self, x):
        x = self.pool(torch.relu(self.conv1(x)))
        x = self.pool(torch.relu(self.conv2(x)))
        x = x.view(-1, 16 * 5 * 5)
        x = self.dropout(torch.relu(self.fc1(x)))  # Dropout applied here
        x = self.dropout(torch.relu(self.fc2(x)))  # Dropout applied here
        x = self.fc3(x)
        return x

# Train model 1 with batch size 64
m1 = Net().to(device)
criterion = nn.CrossEntropyLoss()
optimizer = optim.SGD(m1.parameters(), lr=0.001, momentum=0.9)
scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)  # Learning rate decay
trainloader1 = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)
testloader1 = torch.utils.data.DataLoader(testset, batch_size=1024, shuffle=False)

# Training loop for model 1 with more epochs and learning rate decay
for epoch in range(20):  # Train for more epochs
    for x, y in trainloader1:
        x, y = x.to(device), y.to(device)
        optimizer.zero_grad()
        output = m1(x)
        loss = criterion(output, y)
        loss.backward()
        optimizer.step()
    scheduler.step()  # Apply learning rate decay

# Train model 2 with batch size 1024
m2 = Net().to(device)
optimizer = optim.SGD(m2.parameters(), lr=1e-3, momentum=0.9)
scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)  # Learning rate decay
trainloader2 = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)
testloader2 = torch.utils.data.DataLoader(testset, batch_size=1024, shuffle=False)

# Training loop for model 2 with more epochs and learning rate decay
for epoch in range(20):  # Train for more epochs
    for x, y in trainloader2:
        x, y = x.to(device), y.to(device)
        optimizer.zero_grad()
        output = m2(x)
        loss = criterion(output, y)
        loss.backward()
        optimizer.step()
    scheduler.step()  # Apply learning rate decay

# Interpolate between m1 and m2
alphas = np.linspace(-1, 2, 11)  # Change alpha range from -1 to 2
train_losses = []
test_losses = []
train_accs = []
test_accs = []

for alpha in alphas:
    m_interp = Net().to(device)
    m_interp.load_state_dict({name: alpha * m1.state_dict()[name] + (1 - alpha) * m2.state_dict()[name] for name in m1.state_dict()})

    # Evaluate the interpolated model on the training set
    train_loss = 0
    correct = 0
    with torch.no_grad():
        for x, y in trainloader1:
            x, y = x.to(device), y.to(device)
            output = m_interp(x)
            loss = criterion(output, y)
            train_loss += loss.item()
            _, predicted = torch.max(output, 1)
            correct += (predicted == y).sum().item()
    train_loss /= len(trainloader1)
    train_acc = correct / len(trainset)
    train_losses.append(train_loss)
    train_accs.append(train_acc)

    # Evaluate the interpolated model on the testing set
    test_loss = 0
    correct = 0
    with torch.no_grad():
        for x, y in testloader1:
            x, y = x.to(device), y.to(device)
            output = m_interp(x)
            loss = criterion(output, y)
            test_loss += loss.item()
            _, predicted = torch.max(output, 1)
            correct += (predicted == y).sum().item()
    test_loss /= len(testloader1)
    test_acc = correct / len(testset)
    test_losses.append(test_loss)
    test_accs.append(test_acc)

# Plot the results
plt.figure(figsize=(10, 5))

# Loss plot
plt.subplot(1, 2, 1)
plt.plot(alphas, train_losses, label='Training Loss', color='blue', linestyle='-')
plt.plot(alphas, test_losses, label='Testing Loss', color='orange', linestyle='--')
plt.xlabel('Interpolation Ratio')
plt.ylabel('Loss')
plt.title('Loss vs Interpolation Ratio')
plt.legend()
plt.ylim(0, max(max(train_losses), max(test_losses)) * 1.1)

# Accuracy plot
plt.subplot(1, 2, 2)
plt.plot(alphas, train_accs, label='Training Accuracy', color='green', linestyle='-')
plt.plot(alphas, test_accs, label='Testing Accuracy', color='red', linestyle='--')
plt.xlabel('Interpolation Ratio')
plt.ylabel('Accuracy')
plt.title('Accuracy vs Interpolation Ratio')
plt.legend()

plt.tight_layout()
plt.show()
